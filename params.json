{
  "name": "Assignment 4: Go Further",
  "tagline": "CMU 15-462/662 Assignment 4: Go Further",
  "body": "# # Overview\r\n\r\n\r\n\r\n# Administrative Details\r\n\r\nThe official due date for your project is **Wed 2016-04-27**. A proposal for your project must be submitted by **2016-04-20**.  In particular, the proposal should:\r\n\r\n* Be submitted to us (**by email**) as a web site with a publicly-accessible URL,\r\n* Specify your name (or possibly two names, if you work with a partner)\r\n* Give a brief description of the proposed project and what you hope to achieve—motivating images are extremely valuable here (e.g., \"in a perfect world, the output of our algorithm would look like this photograph...\")\r\n* Give a list of concrete steps you will take to implement the proposed algorithm.\r\n\r\nAs just one \"toy\" example, let's say your goal is to extend the _MeshEdit_ code to implement the most basic version of Catmull-Clark subdivision (which is _not_ enough for a real assignment!).  You might write:\r\n\r\n1. _\"We will first modify the viewer to load and display quadrilateral meshes.\"_\r\n2. _\"We will then implement simple linear subdivision by splitting each polygon at its center.\"_\r\n3. _\"Finally, we will write a routine that computes new vertex positions according to the actual Catmull-Clark routine.\"_\r\n\r\nFor a real project, these bullet points should be _slightly_ bigger and higher-level than the ones in the example above.  But they should still be split enough into small enough chunks that you have a concrete sense of what to do next.  If you're having trouble coming up with a concrete game plan, indicate which steps are giving you trouble and the staff will try to provide some guidance.  Overall, a big part of this project will be developing your skill in starting with a high-level conceptual target, and turning it into concrete, actionable items that you can actually achieve.  (This kind of ability is one key difference between a good developer and a stellar one!)\r\n\r\nFinally, to allow you to implement \"cooler stuff,\" **you may work with a partner if you so desire.**  However, partners must be clearly indicated on your proposal statement and final project submission.\r\n\r\n## Option B: Cartoon Interpolation\r\n\r\n![Deformation of a 2D cartoon]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/cartooninterpolation.png)\r\n\r\nAll of the characters in our 2D animation assignment were drawn as a jumble of disconnected shapes (ellipses, rectangles, polygons, etc.).  For instance, an arm might have been described as a pair of boxes connected by a hinge.  In a real character, of course, you might want a continuous bend at the elbow.  The goal of this project is to enrich the kind of character the system can draw, focusing in particular on continuous deformations.  Here are a couple nice demos of this kind of system:\r\n\r\n* [Spine](http://esotericsoftware.com)\r\n* [Animato](http://cauchy.ch/animato/#/)\r\n\r\nThere are many ways to approach this kind of deformation, many of which can be built on top of the 2D animation code your wrote in your previous assignment.  The simplest way would be to load up an SVG file that contains (i) an image, and (ii) a polygon sitting on top of the image that describes a rough outline or \"cage.\"  The user will manipulate the vertices of the cage, and the system will warp the image in some nice way to produce a continuous deformation of the character.  By animating the polygon vertices, the character can then be animated over time, like any other character in your current system.  (We would advise that you _add_ this functionality on top of existing characters, since these simpler characters may still prove useful for other animation tasks.)\r\n\r\nSo, how do you deform an image according to a polygon?  If your polygon were a triangle, it would be easy: just use linear inteprolation!  For general polygons, there is no one simple answer—in fact, there are a large number of techniques each with different pros and cons.  In some sense, all of these methods are attempting to generalize the notion of _barycentric coordinates_, i.e., how can an arbitrary point on the interior of the polygon be expressed as a combination of the locations of the polygon vertices?  Once you know the answer to this question, deforming the image becomes straightforward.  The simplest way, perhaps, is to tessellate the control polygon into many small triangles; the vertices of the fine triangulation can be updated using the cartoon interpolation scheme, and then the image can be interpolated linearly inside each small triangle (this approach is quite easy to implement in OpenGL).  However, each cartoon interpolation method will have slightly different details and requirements.  Here are a few documents that provide good starting points—some of these papers are quite advanced, but should give a sense of what's possible.  The first link gives a pretty good overview of current methods.  The most useful parts to read are the \"Introduction\" and \"Related Work\" sections, which will give you some sense of which _other_ methods are out there, and what might be easiest to implement for your project:\r\n\r\n* [Skinning: Real-time Shape Deformation](http://skinning.org) - overview course from SIGGRAPH Asia 2014 / SGP 2015\r\n* [Generalized Barycentric Coordinates](http://www.inf.usi.ch/hormann/barycentric/Hormann.pdf) - some overview slides by Kai Hormann (image warping example near the end)\r\n* [Bounded Biharmonic Weights for Real-Time Deformation](http://igl.ethz.ch/projects/bbw/)\r\n* [Smooth Shape-Aware Functions with Controlled Extrema](http://igl.ethz.ch/projects/monotonic/)\r\n* [Planar Shape Interpolation with Bounded Distortion](http://www.eng.biu.ac.il/~weberof/Publications/QC-Interpolation/qc-interpolation.pdf)\r\n* [Bounded Distortion Harmonic Mappings in the Plane](https://people.mpi-inf.mpg.de/~chen/papers/bdhm.pdf)\r\n* [Controllable Conformal Maps for Shape Deformation and Interpolation](http://www.eng.biu.ac.il/~weberof/Publications/Conformal/Controllable_Conformal_Maps_for_Shape_Deformation_and_Interpolation.pdf)\r\n\r\n\r\n## Option D: Font Rendering\r\n\r\n![Gutenberg bible]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/typography.png)\r\n\r\nIn your first assignment, you wrote a rasterizer to draw 2D vector graphics.  An extremely important example of 2D vector graphics is _text!_  In other words, whenever fonts are getting rendered to the screen, some kind of 2D rasterization is needed.  It's easy to dismiss text rendering as a dry, \"solved\" problem, but in reality typography is a beautiful and challenging subject, with a [long and rich history](https://www.youtube.com/watch?v=wOgIkxAfJsk).  It also has a clear impact on the daily lives of billions of people.  One possible project is to extend your rasterization engine to handle type:\r\n\r\n1. _Implement rasterization of cubic and quadratic Bézier curves._  As we studied in one of the quizzes, these curves are needed to render either [TrueType](https://en.wikipedia.org/wiki/TrueType) or [PostScript](https://en.wikipedia.org/wiki/PostScript_fonts) fonts (respectively).  There are two principal ways to render Bézier curves.  One is to [use an incremental algorithm](http://members.chello.at/easyfilter/bresenham.pdf), like the \"Bresenham\" algorithm we studied for lines.  A [more modern way](http://www.msr-waypoint.net/en-us/um/people/cloop/LoopBlinn05.pdf) is to convert explicit Bézier curves into an implicit representation, and evaluate ths implicit function at each sample in a convex region containing the curve; this approach is more amenable to [parallel or GPU implementation](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch25.html). Implement both methods\r\n* _Character layout._  Given a font and a plain-text string of characters, where and how should each character display on screen?  This question is deceptively simple, and has led to generations of ideas in professional typography about kerning, ligatures, and so forth.  To begin with, you could implement Donald Knuth's dynamic programming line-breaking algorithm (full justification) and all the fancy special cases for font ligatures.  Character width will need to be extracted from the font definition (see below).\r\n* Implement a loader for TrueType/PostScript fonts, _or link against an existing one!_  Obviously loading fonts is a critical part of actually rendering them, but perhaps your time is better spent on the rasterization algorithms themselves.  (No reason to reinvent the wheel.)\r\n* A completely different direction you could go is to move past traditional font definitions, and develop/design your own.  For instance, what if you want animated fonts?  Or fonts that encode the random variations found in handwriting?  Can you come up with a nice representation for these \"richer\" fonts?  Should you still use Bézier curves?  Or, given that you already know that **implicit** representations lead to efficient algorithms for curve rasterization (as mentioned above), why not design a font format based on implicit representations from the ground up?  How can you respect history while still moving forward?  Can you add richness to font descriptions without abandoning important and hard-earned knowledge about things like kerning and ligatures?  Just because there's an existing standard doesn't mean you have to adhere to it forever—your job as a thinking, breathing individual is to go out into the world and innovate.\r\n\r\n## Option E: 3D Rasterization\r\n\r\n![Example of order-independent transparency]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/3drasterization.png)\r\n\r\nYour first assignment focused primarily on 2D rasterization, but isn't so far from a full-blown 3D rasterization pipeline.  One possible extension for your final assignment is to implement a more full-featured 3D rasterization engine, in the spirit of OpenGL.  Additional features to consider might include:\r\n\r\n* implement the camera projection matrix\r\n* implement the viewing transform\r\n* perform perspective-correct attribute interpolation (color, texture coordinates, etc.)\r\n* implement [depth testing](https://en.wikipedia.org/wiki/Z-buffering) by adding a Z-buffer\r\n* implement [order-independent transparency](https://en.wikipedia.org/wiki/Order-independent_transparency) using an A-buffer\r\n* implement simple per-pixel lighting/shading\r\n* _advanced:_ implement shadow mapping\r\n* _advanced:_ add trilinear filtering (small change from your assignment 1 implementation to get the partial right)\r\n\r\nOverall, a fun question to think about is: what features can you implement in a software rasterizer that you _cannot_ (easily) implement in a hardware rasterizer.  Roughly speaking, modern graphics hardware is designed around a restricted set of rasterization features that were deemed to be important by the OpenGL standards board.  As a consequence, it can be tricky to do things like implement an A-buffer, which is not directly supported by the hardware.  But that doesn't mean OpenGL represents the \"one true way\" to design a rasterizer.  How does rasterization change in various scenarios?  Does virtual reality (VR) present new challenges for rasterization that don't show up in the traditional pipeline?\r\n\r\nAlso, what about geometric primitives other than triangles?  Graphics hardware is mainly a \"triangle machine,\" meaning that even when you draw a quad, a polygon, and even a point, it gets diced into triangles before being sent to the hardware.  The reason is chip area: if you can build one little piece of hardware that renders triangles, and render everything else in terms of triangles, then you can print a much smaller silicon wafer.  Software rasterizers are fun because they don't inherently have this restriction (i.e., you're not designing a chip, so you don't care about die area!).  So: what other primitives can you rasterize?  Interestingly enough, early NVIDIA chips supported direct rasterization of [bilinear patches](http://shaunramsey.com/research/bp/).  There are likewise algorithms for [visualizing higher-order Bézier patches](https://www.google.com/?gws_rd=ssl#q=ray+tracing+bezier+patches) or [subdivision surfaces](https://www.google.com/search?client=safari&rls=en&q=ray+tracing+subdivision+surfaces&ie=UTF-8&oe=UTF-8) using (you guessed it) ray tracing.  Much like high-performance algorithms for triangle rendering, the idea is to rasterize some region bounding (the projection of) a primitive like a Bézier patch, and then do a ray intersection with an implicit description of the patch to determine visibility, shading, etc.  The (_potential_) advantage from a systems point of view is that you don't have to dice patches into tiny little triangles, which means you don't have to waste as much bandwidth pushing all this data through the pipeline.\r\n\r\nLots of directions to consider here.  What's most exciting to you?\r\n\r\n## Option F: Advanced Rendering and Simulation\r\n\r\n![Advanced rendering example]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/advancedrendering.png)\r\n\r\nFinally, you could extend the physically-based renderer you started to develop for your third assignment—as you experienced first-hand, the basic path tracing algorithm does not converge very quickly!  It also doesn't capture some important (and beautiful!) physical phenomena.  Here are several specific suggestions for directions you might pursue:\r\n\r\n* _Improve performance of BVH construction._  For large and/or dynamic scenes, the cost of building a bounding volume hierarchy is specific.  Consider, for instance, running a large, mesh-based physical simulation of water, which you then want to ray trace in order to get beautiful caustics.  Lots of geometry changing in unpredictable ways—and _millions_ of rays to properly resolve the appearance of the surface.  You could improve the performance of your renderer by implementing one of the the top modern parallel BVH build algorithms such as\r\n\t* [Maximizing Parallelism in the Construction of BVHs, Octrees, and k-d Trees](http://delivery.acm.org/10.1145/2390000/2383801/p33-karras.pdf?ip=128.237.162.63&id=2383801&acc=ACTIVE%20SERVICE&key=A792924B58C015C1%2E5A12BE0369099858%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=768881322&CFTOKEN=58777656&__acm__=1460177293_4beaeb6a169ba31440c963965a84773f)\r\n   * [Fast Parallel Construction of High-Quality Bounding Volume Hierarchies](https://research.nvidia.com/publication/fast-parallel-construction-high-quality-bounding-volume-hierarchies)\r\n   * [tThinking Parallel, Part III: Tree Construction on the GPU](https://devblogs.nvidia.com/parallelforall/thinking-parallel-part-iii-tree-construction-gpu/)\r\n\r\n\t**Note**: If you choose this topic, you can work based on your assignment 3's code. \r\n\t\r\n\tTask Decomposition:\r\n\t\r\n\t1.  Implemented the Morton code based BVH construction method (which runs on CPU) described in [Thinking Parallel, Part III: Tree Construction on the GPU](https://devblogs.nvidia.com/parallelforall/thinking-parallel-part-iii-tree-construction-gpu/)\r\n\t\r\n\t2. Implement the parallel binary radix tree(BRT) construction algorithm as proposed in Karras, Tero. [Maximizing Parallelism in the Construction of BVHs, Octrees, and k-d Trees](http://delivery.acm.org/10.1145/2390000/2383801/p33-karras.pdf?ip=128.237.162.63&id=2383801&acc=ACTIVE%20SERVICE&key=A792924B58C015C1%2E5A12BE0369099858%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=768881322&CFTOKEN=58777656&__acm__=1460177293_4beaeb6a169ba31440c963965a84773f) for general BRT construction\r\n\t\r\n\t3. implemented the parallel algorithm for BVH construction based on BRT construction algorithm, which augments it by adding a bottom-up parallel bounding box construction phase as described in the previous paper.\r\n\r\n* _Improve performance of ray tracing._  There are also plenty of opportunities to improve the performance of the ray tracing itself.  One possibility is to implement fast packet-based ray tracing a la the method described in, [\"Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies\"](http://graphics.stanford.edu/~boulos/papers/togbvh.pdf).\r\n* _SPH Fluid Simulation_. Fluid simulation and rendering enhances the immersive experience of interactive applications like video games and movies. Among many approaches, Smoothed Particle Hydrodynamics(SPH) is arguably the easiest one to implement. In this task, you will need to simulate the fluids particles using SPH model, then reconstruct triangle meshes from the discrete particles using _Marching Cubes_ algorithm. If you want, you can render the scene using some graphics APIs like OpenGL and DirectX. Then you will be surprised by how beautiful the result is. References include:\r\n\t+ Smoothed Particle Hydrodynamics\r\n   \t\t* [Particle-based fluid simulation for interactive applications](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.844&rep=rep1&type=pdf)\r\n\t\t* [Fluid Simulation for Video Games (part 1)](https://software.intel.com/en-us/articles/fluid-simulation-for-video-games-part-1)\r\n\t\t* [Fluid Simulation for Video Games (part 2)](https://software.intel.com/en-us/articles/fluid-simulation-for-video-games-part-2)\r\n\t+ Marching Cube Algorithm\r\n\t\t* [Marching Cubes: A high resolution 3D surface construction algorithm ](http://www.eecs.berkeley.edu/~jrs/meshpapers/LorensenCline.pdf)\r\n\t+ [OpenGL Tutorial](http://www.opengl-tutorial.org/beginners-tutorials/)\r\n\t\r\n\t**Note**: If you choose this topic, we expect you to write everything from scratch. (chanllenging but a lot of fun). \r\n\t\r\n\tTasks Decomposition:\r\n\t\r\n\t1. Implment SPH model to update particle positions/velocities. (At this stage, you can visualize the result by plotting the particles in matlab.)\r\n\t\r\n\t2. Implement Marching Cube algorithm to reconstruct triangle mehes\r\n\t\r\n\t3. Rendering the fluid using OpenGL/DirectX\r\n\t\r\n\r\n## Option G: Iterated Function Systems (IFS)\r\n\r\n![Iterated Function Systems]( https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Barnsley.png?raw=true )\r\n\r\nYou may extend your p1 rasterization program to render fractal images based on Iterated Function Systems. The main idea is that you start with a point and then repeatedly rasterize the point and then apply a distribution of affine transformations to it. This is a wonderful opportunity to create beauty from mathematics. Here are some concrete steps and features you could implement:\r\n\r\n1. _Sierpinski triangle:_ As a warmup you should render a simple IFS such as the [Sierpinski Triangle](https://en.wikipedia.org/wiki/Sierpinski_triangle#Chaos_game). You will need to modify the p1 code to drive the rendering via procedures instead of an SVG file..\r\n2. _Barnsley Fern_ You can then make simple renderings of [Barnsley Ferns](https://en.wikipedia.org/wiki/Barnsley_fern)\r\n3. Extend the SVG for representing IFS based files. You would then need to extend other relevant functions such as the file parser.\r\n* _Irradiance Caching_ If you were to simply plot each of the points, then you would end up with an image looking like a set of dots and would be limited in detail to the rasterization size of a point. Most images you will see on the internet never get past naive plotting. To combat this we can instead create a buffer and instead of plotting points as an opaque color, we plot their relative energies interpolated around the 4 pixels they are plotted upon. You can then compute colors for each pixel based on the irradiance cache's energy divided by the largest energy value. In other words we are mapping all energy values from 'black' to 'white'. You may need to think through you aesthetic sensibilities when deciding upon a background color and the colors on each end of the spectrum.\r\n* _Distance Function_ Now the question is how do we render the background in a way that is aestetically based on the IFS? One solution is to color every background pixel colors based on how far away they are from the set of points defined by the IFS. In other words the shortest distance form the background point to any point that was rasterized inside of the set. This problem is closely related to [voronoi diagrams](https://en.wikipedia.org/wiki/Voronoi_diagram). Here is one efficient way to go about doing this:\r\n   * Create a buffer with one value for every pixel describing how far the given location is from the IFS set. Lets call this the distance buffer.\r\n   * Initialize those locations with non zero energy values in the irradiance cache to have values of 0 in the distance buffer.\r\n   * Creatively propagate distance buffer values using an algorithm similar to flood fill and breadth first search. Every distance buffer location should propagate a pointer to the member of the set they are closest to into their neighboring locations that either have not been initialized yet or have distances larger than the value they would be updated to. You will need to think about which neighbors it will be most efficient to propagate values to, because you want to minimize the amount of times a given value is updated.\r\n   * Color background pixels using the data in the distance buffer. This a great opportunity to experiment with color gradients and interpolation between colors. This is somewhat related to tone mapping.\r\n   * Talk to Bryce and ask him questions about this option.\r\n* _Fractal Flame Algorithm_ Implement the [Fractal Flame Algorithm](http://flam3.com/flame.pdf)\r\n* _Design your own IFS!_ If you are tired of the same old triangles and ferns, you could and should design and formalize your own IFS. Come up with an idea for a shape, derive some affine transformations for it, implement it, render it, stylize it, make something amazing. Be sure to include the mathematics you came up with in your creative process.\r\n\r\n\r\n## Option H: 2D Fluence Renderer\r\n\r\n![Diffuse 2D rendering]( https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Diffuse_Fluence_Rendering.png?raw=true )\r\n\r\nIn Assignment 3 we created images based on the amount of irradiance present on visible surfaces using ray tracing. What would happen if instead we asked how much irradiance passes a given point in space? This is the fluence of a given point and is understandable when visualized in 2D space. For this extension, you can start with either your Ray tracer or your assignment 1 rasterizer, but you will be performing operations on a 2D image buffer while also needing much of the mathematics and structures of your raytracer.\r\n\r\nTo start you off on the right track here are some great resources that you should look at:\r\n* [Secret Life of Photons]( https://benedikt-bitterli.me/tantalum/ )\r\n* [Zen Photon Garden] (http://zenphoton.com/)\r\n* [Transient Rendering] (http://www.cs.dartmouth.edu/~wjarosz/publications/jarabo14framework.html)\r\n\r\nHere are some concrete steps and features you could implement:\r\n* Light Sources : Start rays at light sources defined by a probability distribution of points, much like the light sources in p3. This will allow you to witness all 3 parts of a shadow: [Link](https://en.wikipedia.org/wiki/Umbra,_penumbra_and_antumbra).\r\n* 2D geometry : Compute the representations and intersections for 2D circles and line segments.\r\n* Primary Rays: Emit rays from light sources via antialiased line rasterization via Xiaolin Wu's line algorithm. Have the lines be drawn from the initial point on the light source in a randomly distributed direction until they hit a piece of geometry or the edge of the image. Have the lines plot their interpolated irradiance in a cache. You can then map the range of energy present in the cache to colors in the image. You may want to clamp the energy values at the light source, because otherwise the rest of the scene will always be too dark due to the extreme concentration of light at the light sources.\r\n* 2D Spatial Data Structures: implement some 2D spatial data structures such as axis aligned bounding boxes to accelerate the 2D geometry queries. Can you find optimizations for constructing these data structures in the limited confines of the 2D plane in the visible region that were not feasible in 3D?\r\n* 2D BRDF's and materials: Implement absorption, refraction, reflection brdf's for the pieces of geometry.\r\n* Implement Wavelength dependent refraction via the [Sellmeier Equation] (https://en.wikipedia.org/wiki/Sellmeier_equation) to achieve cool effects like in the Secret Life of Photons.\r\n* Render from an svg: Work on the SVG parser pipeline to specify scenes in svg / sml that may be rendered using your fluence renderer.\r\n* Correct the rasterization bias like in Secret Life of Photons.\r\n* Perform the proper gamma correction. Map different \r\n* [Transient Rendering] (http://www.cs.dartmouth.edu/~wjarosz/publications/jarabo14framework.html) : Create animations of light propagating throughout a scene. Also google Ramesh Raskar and check out his work with [Fempto Photography](https://www.ted.com/talks/ramesh_raskar_a_camera_that_takes_one_trillion_frames_per_second?language=en)\r\n\r\n\r\n## Option I: kPhone 869 (15-869 Assignment)\r\n\r\n![868p source image.](http://graphics.cs.cmu.edu/courses/15869/fall2013content/article_images/6_1.jpg)\r\nYou could implement a simple image processing pipeline for the data produced by the image sensor of the much-anticipated kPhone 869s. (The 's' is for the student version of the phone. It is identical to the 869p, except the gradebook app has been removed, and a voice-assistant app, accessed by speaking 'Ok Yong', has been added.) The 869s has a camera, and your job is to process the data coming off the kPhone 869s's sensor to produce the highest quality image you can. In addition to implementing basic image processing of sensor outputs, you are also be responsible for controlling the focus of the camera!\r\n\r\nThis is an assignment from Kayvon's Visual Computing Systems class 15-869 from fall of 2014.\r\n[Here is a link to the details.](http://graphics.cs.cmu.edu/courses/15869/fall2014/article/3)",
  "google": "",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}