{"name":"Assignment 5: Go Further","tagline":"CMU 15-462/662 Assignment 5: Go Further","body":"# Overview\r\n\r\nIn this final assignment, you will extend some part of the code you developed in the first part of the class in order to make it richer, more interesting, more full-featured, and more fun.  Below we have provided a large number of suggestions, with a few specific starting points.  We suggest that you stay somewhere within the vicinity of these suggestions, since your existing code will make it (_reasonably_) easy to implement many of these algorithms.  However, if you're feeling really passionate about implementing one particular graphics algorithm, we're glad to hear about it!\r\n\r\nNote that we are not requiring you to hit specific technical bullet points—rather, your goal should be to build a more interesting and/or full-featured tool that extends a previous assignment in fun and creative ways.  You can achieve this goal in any way you like.  In other words, we are trying to prep you for building software in the real world, where there is no pre-baked script or list of bullet points you need in order to get \"points.\"  Also note that in many of these projects you will likely have to change some of the viewer/GUI in order to expose this new functionality to the user.  This type of coding can be tricky; if you are having trouble coming up with a good, simple way to modify the GUI, please consult with one of the TAs, who know the system well.  Also, note that code for some functionality (e.g., [Catmull-Clark subdivision](http://rosettacode.org/wiki/Catmull–Clark_subdivision_surface)) is widely available on the web.  Unlike previous assignments, in this project we are **encouraging you** to use existing code, in order to more rapidly build up a more full-featured/interesting/fun tool.  However, we will not give you credit simply for plugging in an existing source file or library!  We want to see that you are building a whole that is greater than the sum of its parts.  So please document and credit appropriately any use of external code or libraries when you hand in the assignment.\r\n\r\n# Administrative Details\r\n\r\nThe official due date for your project is **Friday, December 11 2015**.  However, we will accept submissions without penalty up until Wednesday, December 16 2015.  No further late days will be accepted after this final cutoff date.\r\n\r\nA proposal for your project must be submitted by **Wednesday, December 2nd 2015**.  In particular, the proposal should:\r\n\r\n* Be submitted to us as a web site with a publicly-accessible URL,\r\n* Specify your name (or possibly two names, if you work with a partner)\r\n* Give a brief description of the proposed project and what you hope to achieve—motivating images are extremely valuable here (e.g., \"in a perfect world, the output of our algorithm would look like this photograph...\")\r\n* Give a list of concrete steps you will take to implement the proposed algorithm.\r\n\r\nAs just one \"toy\" example, let's say your goal is to extend the _MeshEdit_ code to implement the most basic version of Catmull-Clark subdivision (which is _not_ enough for a real assignment!).  You might write:\r\n\r\n1. _\"We will first modify the viewer to load and display quadrilateral meshes.\"_\r\n2. _\"We will then implement simple linear subdivision by splitting each polygon at its center.\"_\r\n3. _\"Finally, we will write a routine that computes new vertex positions according to the actual Catmull-Clark routine.\"_\r\n\r\nFor a real project, these bullet points should be _slightly_ bigger and higher-level than the ones in the example above.  But they should still be split enough into small enough chunks that you have a concrete sense of what to do next.  If you're having trouble coming up with a concrete game plan, indicate which steps are giving you trouble and the staff will try to provide some guidance.  Overall, a big part of this project will be developing your skill in starting with a high-level conceptual target, and turning it into concrete, actionable items that you can actually achieve.  (This kind of ability is one key difference between a good developer and a stellar one!)\r\n\r\nFinally, to allow you to implement \"cooler stuff,\" **you may work with a partner if you so desire.**  However, partners must be clearly indicated on your proposal statement and final project submission.\r\n\r\n## Option A: Subdivision Modeler\r\n\r\n![Box modeling example]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/boxmodeling.jpg)\r\n\r\nMany modern 3D modelers are based on subdivision tools that are not much different from the \"MeshEdit\" tool you started to build your earlier assignment.  The basic idea is that the artist starts with a simple primitive like a cube, and adds more and more detail to the model via primitive operations like extruding faces and beveling edges. [Here is one example video](https://youtu.be/HawphRvPusA), but you can find many, many more by simply Googling \"box modeling\" or \"subdivision modeling,\" which you should definitely do if you chose to pursue this project.  You should also take a look at some different modeling packages, such as [Wings3D](http://www.wings3d.com), [Rhino](https://www.rhino3d.com), [modo](https://www.thefoundry.co.uk/products/modo/), [Blender](https://www.blender.org), [Maya](http://www.autodesk.com/products/maya), and [Houdini](http://www.sidefx.com).  Many of these are commercial packages, but still offer a useful glimpse at the world of subdivision modeling.  A few packages (like Blender and Wings3D) are free, and you should try downloading them and playing around with them.\r\n\r\nYour goal will be to develop your mesh editing software into a basic subdivision modeler.  Some directions of extension include:\r\n\r\n* _Additional atomic editing operations._  For instance, in your previous assignment you implemented edge flips, splits, and collapses.  What are some convenient operations for polygon modeling?  Common ones include, say, face extrusion and edge beveling.  But there are many more options, and the goal of this assignment is to play around with the polygon modeling paradigm in new and creative ways.\r\n\r\n* _Additional selection modes._  Almost as important as convenient meshing operations are intelligent selection modes.  For instance, it is very common to allow the user to select a \"loop\" of faces, for things like adding additional detail around the eyes and mouth (you will see this kind of operation in many videos on box modeling).  Adding additional selection modes will require some re-thinking and re-tooling of the viewer itself, but all the fundamental code you need (picking, etc.) is already there.\r\n\r\n* _Support for additional types of subdivision surfaces._  In your assignment you implemented Loop subdivision, which is useful when working with triangle meshes.  However, many natural objects are more easily modeled using quadrilateral meshes, since in general surfaces will have two orthogonal directions of principal curvature.  The half edge mesh class already supports general polygon meshes; you may want to extend your subdivision support to include, say, quad meshes by implementing something like the [Catmull-Clark](https://en.wikipedia.org/wiki/Catmull–Clark_subdivision_surface) subdivision scheme.\r\n\r\n* _Fast preview of subdivided surface_.  Also, it's worth noticing that most professional subdivision tools allow to work on a coarse \"control cage\" while simultaneously getting a preview of what the final subdivided surface will look like.  In other words, the user doesn't actually ever dice the surface into tiny polygons; they simply use the vertices, edges, and faces of the coarse shape as a way to manipulate the smooth subdivided surface.  It would definitely be valuable in your modeler to provide this kind of functionality, either by [approximating subdivision patches with simpler surfaces](http://research.microsoft.com/en-us/um/people/cloop/accTOG.pdf) or simply by coming up with an intelligent scheme to update a fine subdivision mesh whenever the coarse vertices are changed.  One thing to think about here is that, even after _n_ subdivision steps, each subdivided vertex has a linear relationship to the vertices in the control cage.  So, you could precompute a mapping (e.g., a matrix) from coarse positions to fine positions in order to get a fast preview (...until the connectivity of the coarse cage gets updated!).\r\n\r\n## Option B: Cartoon Interpolation\r\n\r\n![Deformation of a 2D cartoon]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/cartooninterpolation.png)\r\n\r\nAll of the characters in our 2D animation assignment were drawn as a jumble of disconnected shapes (ellipses, rectangles, polygons, etc.).  For instance, an arm might have been described as a pair of boxes connected by a hinge.  In a real character, of course, you might want a continuous bend at the elbow.  The goal of this project is to enrich the kind of character the system can draw, focusing in particular on continuous deformations.  Here are a couple nice demos of this kind of system:\r\n\r\n* [Spine](http://esotericsoftware.com)\r\n* [Animato](http://cauchy.ch/animato/#/)\r\n\r\nThere are many ways to approach this kind of deformation, many of which can be built on top of the 2D animation code your wrote in your previous assignment.  The simplest way would be to load up an SVG file that contains (i) an image, and (ii) a polygon sitting on top of the image that describes a rough outline or \"cage.\"  The user will manipulate the vertices of the cage, and the system will warp the image in some nice way to produce a continuous deformation of the character.  By animating the polygon vertices, the character can then be animated over time, like any other character in your current system.  (We would advise that you _add_ this functionality on top of existing characters, since these simpler characters may still prove useful for other animation tasks.)\r\n\r\nSo, how do you deform an image according to a polygon?  If your polygon were a triangle, it would be easy: just use linear inteprolation!  For general polygons, there is no one simple answer—in fact, there are a large number of techniques each with different pros and cons.  In some sense, all of these methods are attempting to generalize the notion of _barycentric coordinates_, i.e., how can an arbitrary point on the interior of the polygon be expressed as a combination of the locations of the polygon vertices?  Once you know the answer to this question, deforming the image becomes straightforward.  The simplest way, perhaps, is to tessellate the control polygon into many small triangles; the vertices of the fine triangulation can be updated using the cartoon interpolation scheme, and then the image can be interpolated linearly inside each small triangle (this approach is quite easy to implement in OpenGL).  However, each cartoon interpolation method will have slightly different details and requirements.  Here are a few documents that provide good starting points—some of these papers are quite advanced, but should give a sense of what's possible.  The first link gives a pretty good overview of current methods.  The most useful parts to read are the \"Introduction\" and \"Related Work\" sections, which will give you some sense of which _other_ methods are out there, and what might be easiest to implement for your project:\r\n\r\n* [Skinning: Real-time Shape Deformation](http://skinning.org) - overview course from SIGGRAPH Asia 2014 / SGP 2015\r\n* [Generalized Barycentric Coordinates](http://www.inf.usi.ch/hormann/barycentric/Hormann.pdf) - some overview slides by Kai Hormann (image warping example near the end)\r\n* [Bounded Biharmonic Weights for Real-Time Deformation](http://igl.ethz.ch/projects/bbw/)\r\n* [Smooth Shape-Aware Functions with Controlled Extrema](http://igl.ethz.ch/projects/monotonic/)\r\n* [Planar Shape Interpolation with Bounded Distortion](http://www.eng.biu.ac.il/~weberof/Publications/QC-Interpolation/qc-interpolation.pdf)\r\n* [Bounded Distortion Harmonic Mappings in the Plane](https://people.mpi-inf.mpg.de/~chen/papers/bdhm.pdf)\r\n* [Controllable Conformal Maps for Shape Deformation and Interpolation](http://www.eng.biu.ac.il/~weberof/Publications/Conformal/Controllable_Conformal_Maps_for_Shape_Deformation_and_Interpolation.pdf)\r\n\r\n## Option C: Mesh-Based Dynamics\r\n\r\n![Wave equation on surfaces]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/meshdynamics.png)\r\n\r\nIn class, we showed a demo of how our three model PDEs (Laplace equation, heat equation, and wave equation) can be used to add interesting surface dynamics to meshes.  In fact, these three linear equations are highly representative of how modern physically-based animation packages simulate interesting phenomena like fluids and elastic bodies.  This project would add mesh-based simulation tools to the mesh editing and rendering code you developed for two of your assignments.  In the end, you would not only be able to load up and animate a mesh, but also create a photorealistic rendering.  For instance, you could simulate waves rippling over a surface, casting caustics onto the rest of the scene.\r\n\r\nHere are several possible directions you could take toward this kind of mesh-based physical animation software:\r\n\r\n* _Implement basic linear equations._  The first step would be to simply implement the basic linear equations (Laplace, heat, and wave equations) on a mesh, using the cotangent formula we studied in class, as well as the ideas we discussed about numerical time integration (splitting, symplectic Euler, etc.).  For instance, in the wave equation it would make sense to store two variables for each vertex of the mesh: the displacement (or \"height\") in the normal direction, as well as the _change_ in displacement over time (i.e., the velocity).  These values could be updated each time step using the symplectic Euler method, and drawn in the viewer by displacing each vertex in the normal direction according to the current height.  (Other things to think about: how do you get proper shading on the surface?  Don't you also have to update the computation of things like surface normals?  Take a look at how the viewer currently works to get a better sense of what's needed here.)\r\n\r\n* _Implement more advanced solvers._ The basic schemes we introducted in class (Jacobi method, explicit time integrators, etc.) will work ok for this kind of simulation, but you may find that they are not particularly fast or stable.  You can improve your system here by implementing a more sophisticated scheme—this shouldn't be too hard, given that the equations you're solving are all linear.  For instance, integrating the heat equation using backward Euler will give you much more stable integration, leading to much larger time steps in practice.  But you may notice that in order to take a backward Euler step, you have to solve a large linear system.  This can be done easily using simple, freely-available linear algebra packages like [Eigen](http://eigen.tuxfamily.org) (which can be added to your project by simply copying and pasting some header files into your project directory!).  From there, you will need to (i) index the vertices of your mesh, (ii) build the Laplace and mass matrice according to these indices, and (iii) solve the linear system that describes the backward Euler update rule.  A lot of these steps are well-documented in [these course notes](http://www.cs.columbia.edu/~keenan/Projects/DGPDEC/paper.pdf) (see especially the section titled \"Meshes and Matrices\").\r\n\r\n* _Implement more interesting equations._ The basic model PDEs are fun to play around with, and already add a lot of additional complexity to your models, but from here there are actually a lot more interesting things you can do.  Here are a few examples:\r\n  * [mesh smoothing](http://w.multires.caltech.edu/pubs/ImplicitFairing.pdf) - A basic task in signal processing in general, and geometry processing in particular, is to remove noise from data by smoothing it out.  One nice approach on meshes is to integrate the so-called _mean curvature flow_.  This PDE looks a lot like the heat equation, except that rather than flowing a normal displacement (just a scalar value at each vertex), you actually flow the (x,y,z) coordinate functions themselves.  Note that in order for this process to make sense, you also have to update the Laplace operator to reflect the geometry of the new surface at each step!\r\n  * [spherical mapping](http://www.cs.jhu.edu/~misha/MyPapers/SGP12.pdf) - At the extreme, if you keep flowing a surface under mean curvature, it will eventually turn into a sphere!  This paper describes a simple-but-effective modification to the above scheme that will more robustly map any surface to a sphere.  These kinds of mappings are important for all sorts of applications, e.g., compressing the geometry of a surface, or comparing the anatomy of two brain scans.  The authors of this method also provide [some nice supplementary information](http://www.cs.jhu.edu/~misha/Code/ConformalizedMCF/).\r\n  * [3d shape editing](http://igl.ethz.ch/projects/deformation-survey/) - Many cutting-edge techniques for shape editing and manipulation are based on solving Laplace-like equations (much like the 2D animation project suggested elsewhere in this assignment).  Basically the idea is that the user provides a few constraints, e.g., a few special points the user wants to move, and the system solves a Laplace-like equation for all the other vertex positions of the mesh.  This kind of tool would be a very easy extension of a working Laplace equation solver; basically you would just have to provide a simple interface for selecting the pinned vertices (the GUI already allows you to click and drag on vertices).  From there, there are many interesting different types of deformation to explore (harmonic, biharmonic, ...); see the link at the beginning of this paragraph for good discussion and pointers.\r\n  * [bubble dynamics](http://www.cs.columbia.edu/cg/doublebubbles/) - This method simulates the way bubbles evolve over time.  The basic flow here is actually not too hard to implement using the type of solver we've described here; most of the complexity in this paper is due to (i) optimization (making things faster), and (ii) dealing with multiple interacting bubbles.  So, you could implement a simplified version that (i) skips some of the difficult optimizations, and (ii) works on only, say, one bubble, or a bubble with interesting (Dirichlet) boundary conditions.\r\n  * [mesh-based surface tension](https://graphics.ethz.ch/publications/papers/paperThue10.php) - This method describes another approach to modeling surface tension, which is an essential ingredient in realistic fluid simulation.\r\n* *Improved rendering.*  Although animation and rendering are often treated as separate topics, there is an important interplay between the two, i.e., when developing algorithms for simulation, it is prudent to think about how this data will ultimately visualized, and how different types of simulation errors and artifacts contribute to (or detract from) the fidelity of the final rendered image.  For instance, if you wish to ray trace a displaced, rippling surface, then the interpolated surface normals will play a role in how well caustics show up (i.e., how smooth and artifact-free they are).  Likewise, if you're really interested in _efficiently_ rendering caustics, you may need to modify your ray tracer to use a scheme like [bidirectional path tracing](https://graphics.stanford.edu/courses/cs348b-03/papers/veach-chapter10.pdf).  Alternatively, if you want to focus more on the simulation itself but bump up the visual fidelity _just a little bit_, you could implement basic [reflection mapping](https://en.wikipedia.org/wiki/Reflection_mapping) using a cube mapped environment map.  (This is easily done in OpenGL on top of the existing viewer code.)\r\n\r\n## Option D: Font Rendering\r\n\r\n![Gutenberg bible]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/typography.png)\r\n\r\nIn your first assignment, you wrote a rasterizer to draw 2D vector graphics.  An extremely important example of 2D vector graphics is _text!_  In other words, whenever fonts are getting rendered to the screen, some kind of 2D rasterization is needed.  It's easy to dismiss text rendering as a dry, \"solved\" problem, but in reality typography is a beautiful and challenging subject, with a [long and rich history](https://www.youtube.com/watch?v=wOgIkxAfJsk).  It also has a clear impact on the daily lives of billions of people.  One possible project is to extend your rasterization engine to handle type.  Possible directions include:\r\n\r\n* _Implement rasterization of cubic and quadratic Bézier curves._  As we studied in one of the quizzes, these curves are needed to render either [TrueType](https://en.wikipedia.org/wiki/TrueType) or [PostScript](https://en.wikipedia.org/wiki/PostScript_fonts) fonts (respectively).  There are two principal ways to render Bézier curves.  One is to [use an incremental algorithm](http://members.chello.at/easyfilter/bresenham.pdf), like the \"Bresenham\" algorithm we studied for lines.  A [more modern way](http://www.msr-waypoint.net/en-us/um/people/cloop/LoopBlinn05.pdf) is to convert explicit Bézier curves into an implicit representation, and evaluate ths implicit function at each sample in a convex region containing the curve; this approach is more amenable to [parallel or GPU implementation](http://http.developer.nvidia.com/GPUGems3/gpugems3_ch25.html).  Finally, here's a [rather old and outdated survey on font rendering](http://lspwww.epfl.ch/publications/typography/frsa.pdf) that is nonetheless nicely written.  (Can you give a more modern summary?)  More generally, this functionality would let you render a broader class of shapes in your 2D rasterization and animation programs.  Fun!\r\n* _Character layout._  Given a font and a plain-text string of characters, where and how should each character display on screen?  This question is deceptively simple, and has led to generations of ideas in professional typography about kerning, ligatures, and so forth.  To begin with, you could implement Donald Knuth's dynamic programming line-breaking algorithm (full justification) and all the fancy special cases for font ligatures.  Character width will need to be extracted from the font definition (see below).\r\n* Implement a loader for TrueType/PostScript fonts, _or link against an existing one!_  Obviously loading fonts is a critical part of actually rendering them, but perhaps your time is better spent on the rasterization algorithms themselves.  (No reason to reinvent the wheel.)\r\n* A completely different direction you could go is to move past traditional font definitions, and develop/design your own.  For instance, what if you want animated fonts?  Or fonts that encode the random variations found in handwriting?  Can you come up with a nice representation for these \"richer\" fonts?  Should you still use Bézier curves?  Or, given that you already know that **implicit** representations lead to efficient algorithms for curve rasterization (as mentioned above), why not design a font format based on implicit representations from the ground up?  How can you respect history while still moving forward?  Can you add richness to font descriptions without abandoning important and hard-earned knowledge about things like kerning and ligatures?  Just because there's an existing standard doesn't mean you have to adhere to it forever—your job as a thinking, breathing individual is to go out into the world and innovate.\r\n\r\n## Option E: 3D Rasterization\r\n\r\n![Example of order-independent transparency]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/3drasterization.png)\r\n\r\nYour first assignment focused primarily on 2D rasterization, but isn't so far from a full-blown 3D rasterization pipeline.  One possible extension for your final assignment is to implement a more full-featured 3D rasterization engine, in the spirit of OpenGL.  Additional features to consider might include:\r\n\r\n* implement the camera projection matrix\r\n* implement the viewing transform\r\n* perform perspective-correct attribute interpolation (color, texture coordinates, etc.)\r\n* implement [depth testing](https://en.wikipedia.org/wiki/Z-buffering) by adding a Z-buffer\r\n* implement [order-independent transparency](https://en.wikipedia.org/wiki/Order-independent_transparency) using an A-buffer\r\n* implement simple per-pixel lighting/shading\r\n* _advanced:_ implement shadow mapping\r\n* _advanced:_ add trilinear filtering (small change from your assignment 1 implementation to get the partial right)\r\n\r\nOverall, a fun question to think about is: what features can you implement in a software rasterizer that you _cannot_ (easily) implement in a hardware rasterizer.  Roughly speaking, modern graphics hardware is designed around a restricted set of rasterization features that were deemed to be important by the OpenGL standards board.  As a consequence, it can be tricky to do things like implement an A-buffer, which is not directly supported by the hardware.  But that doesn't mean OpenGL represents the \"one true way\" to design a rasterizer.  How does rasterization change in various scenarios?  Does virtual reality (VR) present new challenges for rasterization that don't show up in the traditional pipeline?\r\n\r\nAlso, what about geometric primitives other than triangles?  Graphics hardware is mainly a \"triangle machine,\" meaning that even when you draw a quad, a polygon, and even a point, it gets diced into triangles before being sent to the hardware.  The reason is chip area: if you can build one little piece of hardware that renders triangles, and render everything else in terms of triangles, then you can print a much smaller silicon wafer.  Software rasterizers are fun because they don't inherently have this restriction (i.e., you're not designing a chip, so you don't care about die area!).  So: what other primitives can you rasterize?  Interestingly enough, early NVIDIA chips supported direct rasterization of [bilinear patches](http://shaunramsey.com/research/bp/).  There are likewise algorithms for [visualizing higher-order Bézier patches](https://www.google.com/?gws_rd=ssl#q=ray+tracing+bezier+patches) or [subdivision surfaces](https://www.google.com/search?client=safari&rls=en&q=ray+tracing+subdivision+surfaces&ie=UTF-8&oe=UTF-8) using (you guessed it) ray tracing.  Much like high-performance algorithms for triangle rendering, the idea is to rasterize some region bounding (the projection of) a primitive like a Bézier patch, and then do a ray intersection with an implicit description of the patch to determine visibility, shading, etc.  The (_potential_) advantage from a systems point of view is that you don't have to dice patches into tiny little triangles, which means you don't have to waste as much bandwidth pushing all this data through the pipeline.\r\n\r\nLots of directions to consider here.  What's most exciting to you?\r\n\r\n## Option F: Advanced Monte Carlo Rendering\r\n\r\n![Advanced rendering example]( http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/advancedrendering.png)\r\n\r\nFinally, you could extend the physically-based renderer you started to develop for your third assignment—as you experienced first-hand, the basic path tracing algorithm does not converge very quickly!  It also doesn't capture some important (and beautiful!) physical phenomena.  Here are several specific suggestions for directions you might pursue:\r\n\r\n* _Improve performance of BVH construction._  For large and/or dynamic scenes, the cost of building a bounding volume hierarchy is specific.  Consider, for instance, running a large, mesh-based physical simulation of water, which you then want to ray trace in order to get beautiful caustics.  Lots of geometry changing in unpredictable ways—and _millions_ of rays to properly resolve the appearance of the surface.  You could improve the performance of your renderer by implementing one of the the top modern parallel BVH build algorithms such as\r\n   * [Efficient BVH Construction via Approximate Agglomerative Clustering](http://graphics.cs.cmu.edu/projects/aac/)\r\n   * [Fast Parallel Construction of High-Quality Bounding Volume Hierarchies](https://research.nvidia.com/publication/fast-parallel-construction-high-quality-bounding-volume-hierarchies)\r\n* _Improve performance of ray tracing._  There are also plenty of opportunities to improve the performance of the ray tracing itself.  One possibility is to implement fast packet-based ray tracing a la the method described in, [\"Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies\"](http://graphics.stanford.edu/~boulos/papers/togbvh.pdf).\r\n* _Implement an advanced global sampling algorithm._ A very different way to reduce the cost of rendering an image is to improve the sampling strategy.  In other words, since each ray can be very expensive to intersect with the scene, it makes sense to put some effort into finding paths that are likely to carry a lot of \"light.\"  The ultimate goal is to \"beat the clock,\" i.e., your fancy strategy should not only give better estimates of the integral as a function of number of rays cast, but it should really decrease the overall render time for a fixed target level of noise (i.e., an estimate of the variance of your estimator).  We discussed a number of these methods in our _Advanced Rendering and Sampling_ lecture; a few you might consider are given in the following list (many of these methods are excellently described in the [PBRT book](http://www.pbrt.org)):\r\n   * [bidirectional path tracing](https://en.wikipedia.org/wiki/Path_tracing#Bidirectional_path_tracing)\r\n   * [Photon mapping](https://en.wikipedia.org/wiki/Photon_mapping)\r\n   * [Metropolis light transport](https://graphics.stanford.edu/papers/metro/)—this method is quite advanced, though the [\"hypercube\" picture](http://sirkan.iit.bme.hu/~szirmay/paper50_electronic.pdf) we discussed in class can make it a bit easier to implement.\r\n* _Improved sampling patterns_. We also discussed a variety of strategies for picking more intelligent sampling patterns (stratified sampling, low-discrepancy, blue noise, etc.).  You could implement a variety of strategies and, like the previous item, compare the estimated variance of the estimator relative to the wall clock time used to render.\r\n* _Irradiance caching_.  A method that is extremely important in practice (i.e., almost all \"real\" renderers use it) but that we didn't have a chance to discuss in class is _irradiance caching_.  In a nutshell the idea is to say: if illumination is varying slowly, you sometimes get away with interpolating existing samples rather than generating new ones.  Exactly _how_ you generate these samples (and how you interpolate them) is critical to getting good results, and plenty has been written on the subject.  But even a basic irradiance caching strategy could greatly improve the utility of your renderer, in the sense that you can get far smoother images with far fewer samples.\r\n\r\n\r\n## Option G: Iterated Function Systems (IFS)\r\n\r\n![Iterated Function Systems]( https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Barnsley.png?raw=true )\r\n\r\nYou may extend your p1 rasterization program to render fractal images based on Iterated Function Systems. The main idea is that you start with a point and then repeatedly rasterize the point and then apply a distribution of affine transformations to it. This is a wonderful opportunity to create beauty from mathematics. Here are some concrete steps and features you could implement:\r\n\r\n* _Sierpinski triangle_ As a warmup you should render a simple IFS such as the [Sierpinski Triangle](https://en.wikipedia.org/wiki/Sierpinski_triangle#Chaos_game). You will need to modify the p1 code to drive the rendering via procedures instead of an SVG file.\r\n* You might want to extend the SVG for representing IFS based files. You would then need to extend other relevant functions such as the file parser.\r\n* _Barnsley Fern_ You can then make simple renderings of [Barnsley Ferns](https://en.wikipedia.org/wiki/Barnsley_fern)\r\n* _Irradiance Caching_ If you were to simply plot each of the points, then you would end up with an image looking like a set of dots and would be limited in detail to the rasterization size of a point. Most images you will see on the internet never get past naive plotting. To combat this we can instead create a buffer and instead of plotting points as an opaque color, we plot their relative energies interpolated around the 4 pixels they are plotted upon. You can then compute colors for each pixel based on the irradiance cache's energy divided by the largest energy value. In other words we are mapping all energy values from 'black' to 'white'. You may need to think through you aesthetic sensibilities when deciding upon a background color and the colors on each end of the spectrum.\r\n* _Distance Function_ Now the question is how do we render the background in a way that is aestetically based on the IFS? One solution is to color every background pixel colors based on how far away they are from the set of points defined by the IFS. In other words the shortest distance form the background point to any point that was rasterized inside of the set. This problem is closely related to [voronoi diagrams](https://en.wikipedia.org/wiki/Voronoi_diagram). Here is one efficient way to go about doing this:\r\n   * Create a buffer with one value for every pixel describing how far the given location is from the IFS set. Lets call this the distance buffer.\r\n   * Initialize those locations with non zero energy values in the irradiance cache to have values of 0 in the distance buffer.\r\n   * Creatively propagate distance buffer values using an algorithm similar to flood fill and breadth first search. Every distance buffer location should propagate a pointer to the member of the set they are closest to into their neighboring locations that either have not been initialized yet or have distances larger than the value they would be updated to. You will need to think about which neighbors it will be most efficient to propagate values to, because you want to minimize the amount of times a given value is updated.\r\n   * Color background pixels using the data in the distance buffer. This a great opportunity to experiment with color gradients and interpolation between colors. This is somewhat related to tone mapping.\r\n   * Talk to Bryce and ask him questions about this option.\r\n* _Fractal Flame Algorithm_ Implement the [Fractal Flame Algorithm](http://flam3.com/flame.pdf)\r\n* _Design your own IFS!_ If you are tired of the same old triangles and ferns, you could and should design and formalize your own IFS. Come up with an idea for a shape, derive some affine transformations for it, implement it, render it, stylize it, make something amazing. Be sure to include the mathematics you came up with in your creative process.\r\n* _Animate your IFS!_ Create an animation by slowly changing the ifs parameters. You could even support IFS's in your p4 Key Framed Animator!\r\n* _Parallelize your IFS_ Speed up the performance of your IFS rasterizer by parallelizing portions of the computation. Can you implement some of the algorithms in a lock free manner?\r\n\r\n\r\n## Option H: 2D Fluence Renderer\r\n\r\n![Diffuse 2D rendering]( https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Diffuse_Fluence_Rendering.png?raw=true )\r\n\r\nIn Assignment 3 we created images based on the amount of irradiance present on visible surfaces using ray tracing. What would happen if instead we asked how much irradiance passes a given point in space? This is the fluence of a given point and is understandable when visualized in 2D space. For this extension, you can start with either your Ray tracer or your assignment 1 rasterizer, but you will be performing operations on a 2D image buffer while also needing much of the mathematics and structures of your raytracer.\r\n\r\nTo start you off on the right track here are some great resources that you should look at:\r\n* [Secret Life of Photons]( https://benedikt-bitterli.me/tantalum/ )\r\n* [Zen Photon Garden] (http://zenphoton.com/)\r\n* [Transient Rendering] (http://www.cs.dartmouth.edu/~wjarosz/publications/jarabo14framework.html)\r\n\r\nHere are some concrete steps and features you could implement:\r\n* Light Sources : Start rays at light sources defined by a probability distribution of points, much like the light sources in p3. This will allow you to witness all 3 parts of a shadow: [Link](https://en.wikipedia.org/wiki/Umbra,_penumbra_and_antumbra).\r\n* 2D geometry : Compute the representations and intersections for 2D circles and line segments.\r\n* Primary Rays: Emit rays from light sources via antialiased line rasterization via Xiaolin Wu's line algorithm. Have the lines be drawn from the initial point on the light source in a randomly distributed direction until they hit a piece of geometry or the edge of the image. Have the lines plot their interpolated irradiance in a cache. You can then map the range of energy present in the cache to colors in the image. You may want to clamp the energy values at the light source, because otherwise the rest of the scene will always be too dark due to the extreme concentration of light at the light sources.\r\n* 2D Spatial Data Structures: implement some 2D spatial data structures such as axis aligned bounding boxes to accelerate the 2D geometry queries. Can you find optimizations for constructing these data structures in the limited confines of the 2D plane in the visible region that were not feasible in 3D?\r\n* 2D BRDF's and materials: Implement absorption, refraction, reflection brdf's for the pieces of geometry.\r\n* Implement Wavelength dependent refraction via the [Sellmeier Equation] (https://en.wikipedia.org/wiki/Sellmeier_equation) to achieve cool effects like in the Secret Life of Photons.\r\n* Render from an svg: Work on the SVG parser pipeline to specify scenes in svg / sml that may be rendered using your fluence renderer.\r\n* Correct the rasterization bias like in Secret Life of Photons.\r\n* Perform the proper gamma correction. Map different \r\n* [Transient Rendering] (http://www.cs.dartmouth.edu/~wjarosz/publications/jarabo14framework.html) : Create animations of light propagating throughout a scene. Also google Ramesh Raskar and check out his work with [Fempto Photography](https://www.ted.com/talks/ramesh_raskar_a_camera_that_takes_one_trillion_frames_per_second?language=en)\r\n\r\n\r\n## Option I: kPhone 869 (15-869 Assignment)\r\n\r\n![868p source image.](http://graphics.cs.cmu.edu/courses/15869/fall2013content/article_images/6_1.jpg)\r\nYou could implement a simple image processing pipeline for the data produced by the image sensor of the much-anticipated kPhone 869s. (The 's' is for the student version of the phone. It is identical to the 869p, except the gradebook app has been removed, and a voice-assistant app, accessed by speaking 'Ok Yong', has been added.) The 869s has a camera, and your job is to process the data coming off the kPhone 869s's sensor to produce the highest quality image you can. In addition to implementing basic image processing of sensor outputs, you are also be responsible for controlling the focus of the camera!\r\n\r\nThis is an assignment from Kayvon's Visual Computing Systems class 15-869 from fall of 2014.\r\n[Here is a link to the details.](http://graphics.cs.cmu.edu/courses/15869/fall2014/article/3)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}