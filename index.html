<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Assignment 4: Go Further by 462cmu</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Assignment 4: Go Further</h1>
      <h2 class="project-tagline">CMU 15-462/662 Assignment 4: Go Further</h2>
      <a href="https://github.com/462cmu/asst5_gofurther" class="btn">View on GitHub</a>
      <a href="https://github.com/462cmu/asst5_gofurther/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/462cmu/asst5_gofurther/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="-overview" class="anchor" href="#-overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a># Overview</h1>

<h1>
<a id="administrative-details" class="anchor" href="#administrative-details" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Administrative Details</h1>

<p>The official due date for your project is <strong>Wed 2016-04-27</strong>. A proposal for your project must be submitted by <strong>2016-04-20</strong>.  In particular, the proposal should:</p>

<ul>
<li>Be submitted to us (<strong>by email</strong>) as a web site with a publicly-accessible URL,</li>
<li>Specify your name (or possibly two names, if you work with a partner)</li>
<li>Give a brief description of the proposed project and what you hope to achieveâ€”motivating images are extremely valuable here (e.g., "in a perfect world, the output of our algorithm would look like this photograph...")</li>
<li>Give a list of concrete steps you will take to implement the proposed algorithm.</li>
</ul>

<p>As just one "toy" example, let's say your goal is to extend the <em>MeshEdit</em> code to implement the most basic version of Catmull-Clark subdivision (which is <em>not</em> enough for a real assignment!).  You might write:</p>

<ol>
<li><em>"We will first modify the viewer to load and display quadrilateral meshes."</em></li>
<li><em>"We will then implement simple linear subdivision by splitting each polygon at its center."</em></li>
<li><em>"Finally, we will write a routine that computes new vertex positions according to the actual Catmull-Clark routine."</em></li>
</ol>

<p>For a real project, these bullet points should be <em>slightly</em> bigger and higher-level than the ones in the example above.  But they should still be split enough into small enough chunks that you have a concrete sense of what to do next.  If you're having trouble coming up with a concrete game plan, indicate which steps are giving you trouble and the staff will try to provide some guidance.  Overall, a big part of this project will be developing your skill in starting with a high-level conceptual target, and turning it into concrete, actionable items that you can actually achieve.  (This kind of ability is one key difference between a good developer and a stellar one!)</p>

<p>Finally, to allow you to implement "cooler stuff," <strong>you may work with a partner if you so desire.</strong>  However, partners must be clearly indicated on your proposal statement and final project submission.</p>

<h2>
<a id="option-a-subdivision-modeler" class="anchor" href="#option-a-subdivision-modeler" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option A: Subdivision Modeler</h2>

<p><img src="http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/boxmodeling.jpg" alt="Box modeling example"></p>

<p>Many modern 3D modelers are based on subdivision tools that are not much different from the "MeshEdit" tool you started to build your earlier assignment.  The basic idea is that the artist starts with a simple primitive like a cube, and adds more and more detail to the model via primitive operations like extruding faces and beveling edges. <a href="https://youtu.be/HawphRvPusA">Here is one example video</a>, but you can find many, many more by simply Googling "box modeling" or "subdivision modeling," which you should definitely do if you chose to pursue this project.  You should also take a look at some different modeling packages, such as <a href="http://www.wings3d.com">Wings3D</a>, <a href="https://www.rhino3d.com">Rhino</a>, <a href="https://www.thefoundry.co.uk/products/modo/">modo</a>, <a href="https://www.blender.org">Blender</a>, <a href="http://www.autodesk.com/products/maya">Maya</a>, and <a href="http://www.sidefx.com">Houdini</a>.  Many of these are commercial packages, but still offer a useful glimpse at the world of subdivision modeling.  A few packages (like Blender and Wings3D) are free, and you should try downloading them and playing around with them.</p>

<p>Your goal will be to develop your mesh editing software into a basic subdivision modeler.  Some directions of extension include:</p>

<ul>
<li><p><em>Additional atomic editing operations.</em>  For instance, in your previous assignment you implemented edge flips, splits, and collapses.  What are some convenient operations for polygon modeling?  Common ones include, say, face extrusion and edge beveling.  But there are many more options, and the goal of this assignment is to play around with the polygon modeling paradigm in new and creative ways.</p></li>
<li><p><em>Additional selection modes.</em>  Almost as important as convenient meshing operations are intelligent selection modes.  For instance, it is very common to allow the user to select a "loop" of faces, for things like adding additional detail around the eyes and mouth (you will see this kind of operation in many videos on box modeling).  Adding additional selection modes will require some re-thinking and re-tooling of the viewer itself, but all the fundamental code you need (picking, etc.) is already there.</p></li>
<li><p><em>Support for additional types of subdivision surfaces.</em>  In your assignment you implemented Loop subdivision, which is useful when working with triangle meshes.  However, many natural objects are more easily modeled using quadrilateral meshes, since in general surfaces will have two orthogonal directions of principal curvature.  The half edge mesh class already supports general polygon meshes; you may want to extend your subdivision support to include, say, quad meshes by implementing something like the <a href="https://en.wikipedia.org/wiki/Catmull%E2%80%93Clark_subdivision_surface">Catmull-Clark</a> subdivision scheme.</p></li>
<li><p><em>Fast preview of subdivided surface</em>.  Also, it's worth noticing that most professional subdivision tools allow to work on a coarse "control cage" while simultaneously getting a preview of what the final subdivided surface will look like.  In other words, the user doesn't actually ever dice the surface into tiny polygons; they simply use the vertices, edges, and faces of the coarse shape as a way to manipulate the smooth subdivided surface.  It would definitely be valuable in your modeler to provide this kind of functionality, either by <a href="http://research.microsoft.com/en-us/um/people/cloop/accTOG.pdf">approximating subdivision patches with simpler surfaces</a> or simply by coming up with an intelligent scheme to update a fine subdivision mesh whenever the coarse vertices are changed.  One thing to think about here is that, even after <em>n</em> subdivision steps, each subdivided vertex has a linear relationship to the vertices in the control cage.  So, you could precompute a mapping (e.g., a matrix) from coarse positions to fine positions in order to get a fast preview (...until the connectivity of the coarse cage gets updated!).</p></li>
</ul>

<h2>
<a id="option-b-cartoon-interpolation" class="anchor" href="#option-b-cartoon-interpolation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option B: Cartoon Interpolation</h2>

<p><img src="http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/cartooninterpolation.png" alt="Deformation of a 2D cartoon"></p>

<p>All of the characters in our 2D animation assignment were drawn as a jumble of disconnected shapes (ellipses, rectangles, polygons, etc.).  For instance, an arm might have been described as a pair of boxes connected by a hinge.  In a real character, of course, you might want a continuous bend at the elbow.  The goal of this project is to enrich the kind of character the system can draw, focusing in particular on continuous deformations.  Here are a couple nice demos of this kind of system:</p>

<ul>
<li><a href="http://esotericsoftware.com">Spine</a></li>
<li><a href="http://cauchy.ch/animato/#/">Animato</a></li>
</ul>

<p>There are many ways to approach this kind of deformation, many of which can be built on top of the 2D animation code your wrote in your previous assignment.  The simplest way would be to load up an SVG file that contains (i) an image, and (ii) a polygon sitting on top of the image that describes a rough outline or "cage."  The user will manipulate the vertices of the cage, and the system will warp the image in some nice way to produce a continuous deformation of the character.  By animating the polygon vertices, the character can then be animated over time, like any other character in your current system.  (We would advise that you <em>add</em> this functionality on top of existing characters, since these simpler characters may still prove useful for other animation tasks.)</p>

<p>So, how do you deform an image according to a polygon?  If your polygon were a triangle, it would be easy: just use linear inteprolation!  For general polygons, there is no one simple answerâ€”in fact, there are a large number of techniques each with different pros and cons.  In some sense, all of these methods are attempting to generalize the notion of <em>barycentric coordinates</em>, i.e., how can an arbitrary point on the interior of the polygon be expressed as a combination of the locations of the polygon vertices?  Once you know the answer to this question, deforming the image becomes straightforward.  The simplest way, perhaps, is to tessellate the control polygon into many small triangles; the vertices of the fine triangulation can be updated using the cartoon interpolation scheme, and then the image can be interpolated linearly inside each small triangle (this approach is quite easy to implement in OpenGL).  However, each cartoon interpolation method will have slightly different details and requirements.  Here are a few documents that provide good starting pointsâ€”some of these papers are quite advanced, but should give a sense of what's possible.  The first link gives a pretty good overview of current methods.  The most useful parts to read are the "Introduction" and "Related Work" sections, which will give you some sense of which <em>other</em> methods are out there, and what might be easiest to implement for your project:</p>

<ul>
<li>
<a href="http://skinning.org">Skinning: Real-time Shape Deformation</a> - overview course from SIGGRAPH Asia 2014 / SGP 2015</li>
<li>
<a href="http://www.inf.usi.ch/hormann/barycentric/Hormann.pdf">Generalized Barycentric Coordinates</a> - some overview slides by Kai Hormann (image warping example near the end)</li>
<li><a href="http://igl.ethz.ch/projects/bbw/">Bounded Biharmonic Weights for Real-Time Deformation</a></li>
<li><a href="http://igl.ethz.ch/projects/monotonic/">Smooth Shape-Aware Functions with Controlled Extrema</a></li>
<li><a href="http://www.eng.biu.ac.il/%7Eweberof/Publications/QC-Interpolation/qc-interpolation.pdf">Planar Shape Interpolation with Bounded Distortion</a></li>
<li><a href="https://people.mpi-inf.mpg.de/%7Echen/papers/bdhm.pdf">Bounded Distortion Harmonic Mappings in the Plane</a></li>
<li><a href="http://www.eng.biu.ac.il/%7Eweberof/Publications/Conformal/Controllable_Conformal_Maps_for_Shape_Deformation_and_Interpolation.pdf">Controllable Conformal Maps for Shape Deformation and Interpolation</a></li>
</ul>

<h2>
<a id="option-c-mesh-based-dynamics" class="anchor" href="#option-c-mesh-based-dynamics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option C: Mesh-Based Dynamics</h2>

<p><img src="http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/meshdynamics.png" alt="Wave equation on surfaces"></p>

<p>In class, we showed a demo of how our three model PDEs (Laplace equation, heat equation, and wave equation) can be used to add interesting surface dynamics to meshes.  In fact, these three linear equations are highly representative of how modern physically-based animation packages simulate interesting phenomena like fluids and elastic bodies.  This project would add mesh-based simulation tools to the mesh editing and rendering code you developed for two of your assignments.  In the end, you would not only be able to load up and animate a mesh, but also create a photorealistic rendering.  For instance, you could simulate waves rippling over a surface, casting caustics onto the rest of the scene.</p>

<p>Here are several possible directions you could take toward this kind of mesh-based physical animation software:</p>

<ul>
<li><p><em>Implement basic linear equations.</em>  The first step would be to simply implement the basic linear equations (Laplace, heat, and wave equations) on a mesh, using the cotangent formula we studied in class, as well as the ideas we discussed about numerical time integration (splitting, symplectic Euler, etc.).  For instance, in the wave equation it would make sense to store two variables for each vertex of the mesh: the displacement (or "height") in the normal direction, as well as the <em>change</em> in displacement over time (i.e., the velocity).  These values could be updated each time step using the symplectic Euler method, and drawn in the viewer by displacing each vertex in the normal direction according to the current height.  (Other things to think about: how do you get proper shading on the surface?  Don't you also have to update the computation of things like surface normals?  Take a look at how the viewer currently works to get a better sense of what's needed here.)</p></li>
<li><p><em>Implement more advanced solvers.</em> The basic schemes we introducted in class (Jacobi method, explicit time integrators, etc.) will work ok for this kind of simulation, but you may find that they are not particularly fast or stable.  You can improve your system here by implementing a more sophisticated schemeâ€”this shouldn't be too hard, given that the equations you're solving are all linear.  For instance, integrating the heat equation using backward Euler will give you much more stable integration, leading to much larger time steps in practice.  But you may notice that in order to take a backward Euler step, you have to solve a large linear system.  This can be done easily using simple, freely-available linear algebra packages like <a href="http://eigen.tuxfamily.org">Eigen</a> (which can be added to your project by simply copying and pasting some header files into your project directory!).  From there, you will need to (i) index the vertices of your mesh, (ii) build the Laplace and mass matrice according to these indices, and (iii) solve the linear system that describes the backward Euler update rule.  A lot of these steps are well-documented in <a href="http://www.cs.columbia.edu/%7Ekeenan/Projects/DGPDEC/paper.pdf">these course notes</a> (see especially the section titled "Meshes and Matrices").</p></li>
<li>
<p><em>Implement more interesting equations.</em> The basic model PDEs are fun to play around with, and already add a lot of additional complexity to your models, but from here there are actually a lot more interesting things you can do.  Here are a few examples:</p>

<ul>
<li>
<a href="http://w.multires.caltech.edu/pubs/ImplicitFairing.pdf">mesh smoothing</a> - A basic task in signal processing in general, and geometry processing in particular, is to remove noise from data by smoothing it out.  One nice approach on meshes is to integrate the so-called <em>mean curvature flow</em>.  This PDE looks a lot like the heat equation, except that rather than flowing a normal displacement (just a scalar value at each vertex), you actually flow the (x,y,z) coordinate functions themselves.  Note that in order for this process to make sense, you also have to update the Laplace operator to reflect the geometry of the new surface at each step!</li>
<li>
<a href="http://www.cs.jhu.edu/%7Emisha/MyPapers/SGP12.pdf">spherical mapping</a> - At the extreme, if you keep flowing a surface under mean curvature, it will eventually turn into a sphere!  This paper describes a simple-but-effective modification to the above scheme that will more robustly map any surface to a sphere.  These kinds of mappings are important for all sorts of applications, e.g., compressing the geometry of a surface, or comparing the anatomy of two brain scans.  The authors of this method also provide <a href="http://www.cs.jhu.edu/%7Emisha/Code/ConformalizedMCF/">some nice supplementary information</a>.</li>
<li>
<a href="http://igl.ethz.ch/projects/deformation-survey/">3d shape editing</a> - Many cutting-edge techniques for shape editing and manipulation are based on solving Laplace-like equations (much like the 2D animation project suggested elsewhere in this assignment).  Basically the idea is that the user provides a few constraints, e.g., a few special points the user wants to move, and the system solves a Laplace-like equation for all the other vertex positions of the mesh.  This kind of tool would be a very easy extension of a working Laplace equation solver; basically you would just have to provide a simple interface for selecting the pinned vertices (the GUI already allows you to click and drag on vertices).  From there, there are many interesting different types of deformation to explore (harmonic, biharmonic, ...); see the link at the beginning of this paragraph for good discussion and pointers.</li>
<li>
<a href="http://www.cs.columbia.edu/cg/doublebubbles/">bubble dynamics</a> - This method simulates the way bubbles evolve over time.  The basic flow here is actually not too hard to implement using the type of solver we've described here; most of the complexity in this paper is due to (i) optimization (making things faster), and (ii) dealing with multiple interacting bubbles.  So, you could implement a simplified version that (i) skips some of the difficult optimizations, and (ii) works on only, say, one bubble, or a bubble with interesting (Dirichlet) boundary conditions.</li>
<li>
<a href="https://graphics.ethz.ch/publications/papers/paperThue10.php">mesh-based surface tension</a> - This method describes another approach to modeling surface tension, which is an essential ingredient in realistic fluid simulation.</li>
</ul>
</li>
<li>
<em>Improved rendering.</em>  Although animation and rendering are often treated as separate topics, there is an important interplay between the two, i.e., when developing algorithms for simulation, it is prudent to think about how this data will ultimately visualized, and how different types of simulation errors and artifacts contribute to (or detract from) the fidelity of the final rendered image.  For instance, if you wish to ray trace a displaced, rippling surface, then the interpolated surface normals will play a role in how well caustics show up (i.e., how smooth and artifact-free they are).  Likewise, if you're really interested in <em>efficiently</em> rendering caustics, you may need to modify your ray tracer to use a scheme like <a href="https://graphics.stanford.edu/courses/cs348b-03/papers/veach-chapter10.pdf">bidirectional path tracing</a>.  Alternatively, if you want to focus more on the simulation itself but bump up the visual fidelity <em>just a little bit</em>, you could implement basic <a href="https://en.wikipedia.org/wiki/Reflection_mapping">reflection mapping</a> using a cube mapped environment map.  (This is easily done in OpenGL on top of the existing viewer code.)</li>
</ul>

<h2>
<a id="option-d-font-rendering" class="anchor" href="#option-d-font-rendering" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option D: Font Rendering</h2>

<p><img src="http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/typography.png" alt="Gutenberg bible"></p>

<p>In your first assignment, you wrote a rasterizer to draw 2D vector graphics.  An extremely important example of 2D vector graphics is <em>text!</em>  In other words, whenever fonts are getting rendered to the screen, some kind of 2D rasterization is needed.  It's easy to dismiss text rendering as a dry, "solved" problem, but in reality typography is a beautiful and challenging subject, with a <a href="https://www.youtube.com/watch?v=wOgIkxAfJsk">long and rich history</a>.  It also has a clear impact on the daily lives of billions of people.  One possible project is to extend your rasterization engine to handle type.  Possible directions include:</p>

<ul>
<li>
<em>Implement rasterization of cubic and quadratic BÃ©zier curves.</em>  As we studied in one of the quizzes, these curves are needed to render either <a href="https://en.wikipedia.org/wiki/TrueType">TrueType</a> or <a href="https://en.wikipedia.org/wiki/PostScript_fonts">PostScript</a> fonts (respectively).  There are two principal ways to render BÃ©zier curves.  One is to <a href="http://members.chello.at/easyfilter/bresenham.pdf">use an incremental algorithm</a>, like the "Bresenham" algorithm we studied for lines.  A <a href="http://www.msr-waypoint.net/en-us/um/people/cloop/LoopBlinn05.pdf">more modern way</a> is to convert explicit BÃ©zier curves into an implicit representation, and evaluate ths implicit function at each sample in a convex region containing the curve; this approach is more amenable to <a href="http://http.developer.nvidia.com/GPUGems3/gpugems3_ch25.html">parallel or GPU implementation</a>.  Finally, here's a <a href="http://lspwww.epfl.ch/publications/typography/frsa.pdf">rather old and outdated survey on font rendering</a> that is nonetheless nicely written.  (Can you give a more modern summary?)  More generally, this functionality would let you render a broader class of shapes in your 2D rasterization and animation programs.  Fun!</li>
<li>
<em>Character layout.</em>  Given a font and a plain-text string of characters, where and how should each character display on screen?  This question is deceptively simple, and has led to generations of ideas in professional typography about kerning, ligatures, and so forth.  To begin with, you could implement Donald Knuth's dynamic programming line-breaking algorithm (full justification) and all the fancy special cases for font ligatures.  Character width will need to be extracted from the font definition (see below).</li>
<li>Implement a loader for TrueType/PostScript fonts, <em>or link against an existing one!</em>  Obviously loading fonts is a critical part of actually rendering them, but perhaps your time is better spent on the rasterization algorithms themselves.  (No reason to reinvent the wheel.)</li>
<li>A completely different direction you could go is to move past traditional font definitions, and develop/design your own.  For instance, what if you want animated fonts?  Or fonts that encode the random variations found in handwriting?  Can you come up with a nice representation for these "richer" fonts?  Should you still use BÃ©zier curves?  Or, given that you already know that <strong>implicit</strong> representations lead to efficient algorithms for curve rasterization (as mentioned above), why not design a font format based on implicit representations from the ground up?  How can you respect history while still moving forward?  Can you add richness to font descriptions without abandoning important and hard-earned knowledge about things like kerning and ligatures?  Just because there's an existing standard doesn't mean you have to adhere to it foreverâ€”your job as a thinking, breathing individual is to go out into the world and innovate.</li>
</ul>

<h2>
<a id="option-e-3d-rasterization" class="anchor" href="#option-e-3d-rasterization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option E: 3D Rasterization</h2>

<p><img src="http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/3drasterization.png" alt="Example of order-independent transparency"></p>

<p>Your first assignment focused primarily on 2D rasterization, but isn't so far from a full-blown 3D rasterization pipeline.  One possible extension for your final assignment is to implement a more full-featured 3D rasterization engine, in the spirit of OpenGL.  Additional features to consider might include:</p>

<ul>
<li>implement the camera projection matrix</li>
<li>implement the viewing transform</li>
<li>perform perspective-correct attribute interpolation (color, texture coordinates, etc.)</li>
<li>implement <a href="https://en.wikipedia.org/wiki/Z-buffering">depth testing</a> by adding a Z-buffer</li>
<li>implement <a href="https://en.wikipedia.org/wiki/Order-independent_transparency">order-independent transparency</a> using an A-buffer</li>
<li>implement simple per-pixel lighting/shading</li>
<li>
<em>advanced:</em> implement shadow mapping</li>
<li>
<em>advanced:</em> add trilinear filtering (small change from your assignment 1 implementation to get the partial right)</li>
</ul>

<p>Overall, a fun question to think about is: what features can you implement in a software rasterizer that you <em>cannot</em> (easily) implement in a hardware rasterizer.  Roughly speaking, modern graphics hardware is designed around a restricted set of rasterization features that were deemed to be important by the OpenGL standards board.  As a consequence, it can be tricky to do things like implement an A-buffer, which is not directly supported by the hardware.  But that doesn't mean OpenGL represents the "one true way" to design a rasterizer.  How does rasterization change in various scenarios?  Does virtual reality (VR) present new challenges for rasterization that don't show up in the traditional pipeline?</p>

<p>Also, what about geometric primitives other than triangles?  Graphics hardware is mainly a "triangle machine," meaning that even when you draw a quad, a polygon, and even a point, it gets diced into triangles before being sent to the hardware.  The reason is chip area: if you can build one little piece of hardware that renders triangles, and render everything else in terms of triangles, then you can print a much smaller silicon wafer.  Software rasterizers are fun because they don't inherently have this restriction (i.e., you're not designing a chip, so you don't care about die area!).  So: what other primitives can you rasterize?  Interestingly enough, early NVIDIA chips supported direct rasterization of <a href="http://shaunramsey.com/research/bp/">bilinear patches</a>.  There are likewise algorithms for <a href="https://www.google.com/?gws_rd=ssl#q=ray+tracing+bezier+patches">visualizing higher-order BÃ©zier patches</a> or <a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=ray+tracing+subdivision+surfaces&amp;ie=UTF-8&amp;oe=UTF-8">subdivision surfaces</a> using (you guessed it) ray tracing.  Much like high-performance algorithms for triangle rendering, the idea is to rasterize some region bounding (the projection of) a primitive like a BÃ©zier patch, and then do a ray intersection with an implicit description of the patch to determine visibility, shading, etc.  The (<em>potential</em>) advantage from a systems point of view is that you don't have to dice patches into tiny little triangles, which means you don't have to waste as much bandwidth pushing all this data through the pipeline.</p>

<p>Lots of directions to consider here.  What's most exciting to you?</p>

<h2>
<a id="option-f-advanced-rendering-and-simulation" class="anchor" href="#option-f-advanced-rendering-and-simulation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option F: Advanced Rendering and Simulation</h2>

<p><img src="http://15462.courses.cs.cmu.edu/fall2015content/misc/asst5_images/advancedrendering.png" alt="Advanced rendering example"></p>

<p>Finally, you could extend the physically-based renderer you started to develop for your third assignmentâ€”as you experienced first-hand, the basic path tracing algorithm does not converge very quickly!  It also doesn't capture some important (and beautiful!) physical phenomena.  Here are several specific suggestions for directions you might pursue:</p>

<ul>
<li>
<p><em>Improve performance of BVH construction.</em>  For large and/or dynamic scenes, the cost of building a bounding volume hierarchy is specific.  Consider, for instance, running a large, mesh-based physical simulation of water, which you then want to ray trace in order to get beautiful caustics.  Lots of geometry changing in unpredictable waysâ€”and <em>millions</em> of rays to properly resolve the appearance of the surface.  You could improve the performance of your renderer by implementing one of the the top modern parallel BVH build algorithms such as</p>

<ul>
<li><a href="http://delivery.acm.org/10.1145/2390000/2383801/p33-karras.pdf?ip=128.237.162.63&amp;id=2383801&amp;acc=ACTIVE%20SERVICE&amp;key=A792924B58C015C1%2E5A12BE0369099858%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=768881322&amp;CFTOKEN=58777656&amp;__acm__=1460177293_4beaeb6a169ba31440c963965a84773f">Maximizing Parallelism in the Construction of BVHs, Octrees, and k-d Trees</a></li>
<li><a href="https://research.nvidia.com/publication/fast-parallel-construction-high-quality-bounding-volume-hierarchies">Fast Parallel Construction of High-Quality Bounding Volume Hierarchies</a></li>
<li><a href="https://devblogs.nvidia.com/parallelforall/thinking-parallel-part-iii-tree-construction-gpu/">tThinking Parallel, Part III: Tree Construction on the GPU</a></li>
</ul>

<p><strong>Note</strong>: If you choose this topic, you can work based on your assignment 3's code. </p>

<p>Task Decomposition:</p>

<ol>
<li><p>Implemented the Morton code based BVH construction method (which runs on CPU) described in <a href="https://devblogs.nvidia.com/parallelforall/thinking-parallel-part-iii-tree-construction-gpu/">Thinking Parallel, Part III: Tree Construction on the GPU</a></p></li>
<li><p>Implement the parallel binary radix tree(BRT) construction algorithm as proposed in Karras, Tero. <a href="http://delivery.acm.org/10.1145/2390000/2383801/p33-karras.pdf?ip=128.237.162.63&amp;id=2383801&amp;acc=ACTIVE%20SERVICE&amp;key=A792924B58C015C1%2E5A12BE0369099858%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=768881322&amp;CFTOKEN=58777656&amp;__acm__=1460177293_4beaeb6a169ba31440c963965a84773f">Maximizing Parallelism in the Construction of BVHs, Octrees, and k-d Trees</a> for general BRT construction</p></li>
<li><p>implemented the parallel algorithm for BVH construction based on BRT construction algorithm, which augments it by adding a bottom-up parallel bounding box construction phase as described in the previous paper.</p></li>
</ol>
</li>
<li><p><em>Improve performance of ray tracing.</em>  There are also plenty of opportunities to improve the performance of the ray tracing itself.  One possibility is to implement fast packet-based ray tracing a la the method described in, <a href="http://graphics.stanford.edu/%7Eboulos/papers/togbvh.pdf">"Ray Tracing Deformable Scenes using Dynamic Bounding Volume Hierarchies"</a>.</p></li>
<li>
<p><em>SPH Fluid Simulation</em>. Fluid simulation and rendering enhances the immersive experience of interactive applications like video games and movies. Among many approaches, Smoothed Particle Hydrodynamics(SPH) is arguably the easiest one to implement. In this task, you will need to simulate the fluids particles using SPH model, then reconstruct triangle meshes from the discrete particles using <em>Marching Cubes</em> algorithm. If you want, you can render the scene using some graphics APIs like OpenGL and DirectX. Then you will be surprised by how beautiful the result is. References include:</p>

<ul>
<li>Smoothed Particle Hydrodynamics

<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.844&amp;rep=rep1&amp;type=pdf">Particle-based fluid simulation for interactive applications</a></li>
<li><a href="https://software.intel.com/en-us/articles/fluid-simulation-for-video-games-part-1">Fluid Simulation for Video Games (part 1)</a></li>
<li><a href="https://software.intel.com/en-us/articles/fluid-simulation-for-video-games-part-2">Fluid Simulation for Video Games (part 2)</a></li>
</ul>
</li>
<li>Marching Cube Algorithm

<ul>
<li><a href="http://www.eecs.berkeley.edu/%7Ejrs/meshpapers/LorensenCline.pdf">Marching Cubes: A high resolution 3D surface construction algorithm </a></li>
</ul>
</li>
<li><a href="http://www.opengl-tutorial.org/beginners-tutorials/">OpenGL Tutorial</a></li>
</ul>

<p><strong>Note</strong>: If you choose this topic, we expect you to write everything from scratch. (chanllenging but a lot of fun). </p>

<p>Tasks Decomposition:</p>

<ol>
<li><p>Implment SPH model to update particle positions/velocities. (At this stage, you can visualize the result by plotting the particles in matlab.)</p></li>
<li><p>Implement Marching Cube algorithm to reconstruct triangle mehes</p></li>
<li><p>Rendering the fluid using OpenGL/DirectX</p></li>
</ol>
</li>
</ul>

<h2>
<a id="option-g-iterated-function-systems-ifs" class="anchor" href="#option-g-iterated-function-systems-ifs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option G: Iterated Function Systems (IFS)</h2>

<p><img src="https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Barnsley.png?raw=true" alt="Iterated Function Systems"></p>

<p>You may extend your p1 rasterization program to render fractal images based on Iterated Function Systems. The main idea is that you start with a point and then repeatedly rasterize the point and then apply a distribution of affine transformations to it. This is a wonderful opportunity to create beauty from mathematics. Here are some concrete steps and features you could implement:</p>

<ol>
<li>
<em>Sierpinski triangle:</em> As a warmup you should render a simple IFS such as the <a href="https://en.wikipedia.org/wiki/Sierpinski_triangle#Chaos_game">Sierpinski Triangle</a>. You will need to modify the p1 code to drive the rendering via procedures instead of an SVG file..</li>
<li>
<em>Barnsley Fern</em> You can then make simple renderings of <a href="https://en.wikipedia.org/wiki/Barnsley_fern">Barnsley Ferns</a>
</li>
<li>You might want to extend the SVG for representing IFS based files. You would then need to extend other relevant functions such as the file parser.</li>
<li>
<em>Irradiance Caching</em> If you were to simply plot each of the points, then you would end up with an image looking like a set of dots and would be limited in detail to the rasterization size of a point. Most images you will see on the internet never get past naive plotting. To combat this we can instead create a buffer and instead of plotting points as an opaque color, we plot their relative energies interpolated around the 4 pixels they are plotted upon. You can then compute colors for each pixel based on the irradiance cache's energy divided by the largest energy value. In other words we are mapping all energy values from 'black' to 'white'. You may need to think through you aesthetic sensibilities when deciding upon a background color and the colors on each end of the spectrum.</li>
<li>
<em>Distance Function</em> Now the question is how do we render the background in a way that is aestetically based on the IFS? One solution is to color every background pixel colors based on how far away they are from the set of points defined by the IFS. In other words the shortest distance form the background point to any point that was rasterized inside of the set. This problem is closely related to <a href="https://en.wikipedia.org/wiki/Voronoi_diagram">voronoi diagrams</a>. Here is one efficient way to go about doing this:

<ul>
<li>Create a buffer with one value for every pixel describing how far the given location is from the IFS set. Lets call this the distance buffer.</li>
<li>Initialize those locations with non zero energy values in the irradiance cache to have values of 0 in the distance buffer.</li>
<li>Creatively propagate distance buffer values using an algorithm similar to flood fill and breadth first search. Every distance buffer location should propagate a pointer to the member of the set they are closest to into their neighboring locations that either have not been initialized yet or have distances larger than the value they would be updated to. You will need to think about which neighbors it will be most efficient to propagate values to, because you want to minimize the amount of times a given value is updated.</li>
<li>Color background pixels using the data in the distance buffer. This a great opportunity to experiment with color gradients and interpolation between colors. This is somewhat related to tone mapping.</li>
<li>Talk to Bryce and ask him questions about this option.</li>
</ul>
</li>
<li>
<em>Fractal Flame Algorithm</em> Implement the <a href="http://flam3.com/flame.pdf">Fractal Flame Algorithm</a>
</li>
<li>
<em>Design your own IFS!</em> If you are tired of the same old triangles and ferns, you could and should design and formalize your own IFS. Come up with an idea for a shape, derive some affine transformations for it, implement it, render it, stylize it, make something amazing. Be sure to include the mathematics you came up with in your creative process.</li>
<li>
<em>Animate your IFS!</em> Create an animation by slowly changing the ifs parameters. You could even support IFS's in your p4 Key Framed Animator!</li>
<li>
<em>Parallelize your IFS</em> Speed up the performance of your IFS rasterizer by parallelizing portions of the computation. Can you implement some of the algorithms in a lock free manner?</li>
</ol>

<h2>
<a id="option-h-2d-fluence-renderer" class="anchor" href="#option-h-2d-fluence-renderer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option H: 2D Fluence Renderer</h2>

<p><img src="https://github.com/Bryce-Summers/Bryce-Summers.github.io/blob/master/Images/Diffuse_Fluence_Rendering.png?raw=true" alt="Diffuse 2D rendering"></p>

<p>In Assignment 3 we created images based on the amount of irradiance present on visible surfaces using ray tracing. What would happen if instead we asked how much irradiance passes a given point in space? This is the fluence of a given point and is understandable when visualized in 2D space. For this extension, you can start with either your Ray tracer or your assignment 1 rasterizer, but you will be performing operations on a 2D image buffer while also needing much of the mathematics and structures of your raytracer.</p>

<p>To start you off on the right track here are some great resources that you should look at:</p>

<ul>
<li><a href="https://benedikt-bitterli.me/tantalum/">Secret Life of Photons</a></li>
<li><a href="http://zenphoton.com/">Zen Photon Garden</a></li>
<li><a href="http://www.cs.dartmouth.edu/%7Ewjarosz/publications/jarabo14framework.html">Transient Rendering</a></li>
</ul>

<p>Here are some concrete steps and features you could implement:</p>

<ul>
<li>Light Sources : Start rays at light sources defined by a probability distribution of points, much like the light sources in p3. This will allow you to witness all 3 parts of a shadow: <a href="https://en.wikipedia.org/wiki/Umbra,_penumbra_and_antumbra">Link</a>.</li>
<li>2D geometry : Compute the representations and intersections for 2D circles and line segments.</li>
<li>Primary Rays: Emit rays from light sources via antialiased line rasterization via Xiaolin Wu's line algorithm. Have the lines be drawn from the initial point on the light source in a randomly distributed direction until they hit a piece of geometry or the edge of the image. Have the lines plot their interpolated irradiance in a cache. You can then map the range of energy present in the cache to colors in the image. You may want to clamp the energy values at the light source, because otherwise the rest of the scene will always be too dark due to the extreme concentration of light at the light sources.</li>
<li>2D Spatial Data Structures: implement some 2D spatial data structures such as axis aligned bounding boxes to accelerate the 2D geometry queries. Can you find optimizations for constructing these data structures in the limited confines of the 2D plane in the visible region that were not feasible in 3D?</li>
<li>2D BRDF's and materials: Implement absorption, refraction, reflection brdf's for the pieces of geometry.</li>
<li>Implement Wavelength dependent refraction via the <a href="https://en.wikipedia.org/wiki/Sellmeier_equation">Sellmeier Equation</a> to achieve cool effects like in the Secret Life of Photons.</li>
<li>Render from an svg: Work on the SVG parser pipeline to specify scenes in svg / sml that may be rendered using your fluence renderer.</li>
<li>Correct the rasterization bias like in Secret Life of Photons.</li>
<li>Perform the proper gamma correction. Map different </li>
<li>
<a href="http://www.cs.dartmouth.edu/%7Ewjarosz/publications/jarabo14framework.html">Transient Rendering</a> : Create animations of light propagating throughout a scene. Also google Ramesh Raskar and check out his work with <a href="https://www.ted.com/talks/ramesh_raskar_a_camera_that_takes_one_trillion_frames_per_second?language=en">Fempto Photography</a>
</li>
</ul>

<h2>
<a id="option-i-kphone-869-15-869-assignment" class="anchor" href="#option-i-kphone-869-15-869-assignment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Option I: kPhone 869 (15-869 Assignment)</h2>

<p><img src="http://graphics.cs.cmu.edu/courses/15869/fall2013content/article_images/6_1.jpg" alt="868p source image.">
You could implement a simple image processing pipeline for the data produced by the image sensor of the much-anticipated kPhone 869s. (The 's' is for the student version of the phone. It is identical to the 869p, except the gradebook app has been removed, and a voice-assistant app, accessed by speaking 'Ok Yong', has been added.) The 869s has a camera, and your job is to process the data coming off the kPhone 869s's sensor to produce the highest quality image you can. In addition to implementing basic image processing of sensor outputs, you are also be responsible for controlling the focus of the camera!</p>

<p>This is an assignment from Kayvon's Visual Computing Systems class 15-869 from fall of 2014.
<a href="http://graphics.cs.cmu.edu/courses/15869/fall2014/article/3">Here is a link to the details.</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/462cmu/asst5_gofurther">Assignment 4: Go Further</a> is maintained by <a href="https://github.com/462cmu">462cmu</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
